{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1cb18a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root added to path: d:\\workspace\\projects\\freelance\\ReconService\n"
     ]
    }
   ],
   "source": [
    "import confnotebook\n",
    "\n",
    "from pullenti.Sdk import Sdk\n",
    "\n",
    "Sdk.initialize_all() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a395ef6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TestExternalOrganization(text_raw='\\n        Ух\\n        Акт сверки расчетов от 30.09.2023\\n        Акционерное общество \"ЕВРОСИБЭНЕРГО\", именуемое в дальнейшем «Продавец» в лице Вилькевичене H.H., действующего на основании доверенности № 84/22 от 16.11.2022 с одной стороны, и Акционерное общество \"РУСАЛ НОВОКУЗНЕЦКИЙ АЛЮМИНИЕВЫЙ ЗАВОД\", именуемое в дальнейшем «Покупатель», в лице ‚ действующего на основании ___, с друсой стороны, составили настоящий Акт сверки расчетов по договору KON-30015094-EVROSENG-NKUZALUM-23-VV-2 от 24.09.2019\\n        руб\\n        От Покупателя\\n        От Продавца / Вилькевичене Н.Н. / по доверенности № 84/22 от 16.11.2022\\n        ', seller='ЕВРОСИБЭНЕРГО, АО', buyer='Русал Новокузнецкий Алюминиевый Завод, АО')\n"
     ]
    }
   ],
   "source": [
    "from tests.test_data_ext_organization import TEST_TEXT_LIST\n",
    "\n",
    "print(TEST_TEXT_LIST[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4cb6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pullenti.ner.ExtOntology import ExtOntology\n",
    "from pullenti.ner.ProcessorService import ProcessorService\n",
    "from pullenti.ner.AnalysisResult import AnalysisResult\n",
    "from pullenti.ner.SourceOfAnalysis import SourceOfAnalysis\n",
    "from pullenti.ner.org.OrganizationAnalyzer import OrganizationAnalyzer\n",
    "from pullenti.ner.org.OrganizationReferent import OrganizationReferent\n",
    "\n",
    "def _configure_org_ontology() -> ExtOntology:\n",
    "    \"\"\"Настраивает онтологию организаций с предопределенными организациями\"\"\"\n",
    "    org_ontos = ExtOntology()\n",
    "    map_orgs = {\n",
    "        'РУСАЛ НОВОКУЗНЕЦКИЙ АЛЮМИНИЕВЫЙ ЗАВОД': 'АО',\n",
    "        'РУСАЛ АЧИНСКИЙ ГЛИНОЗЕМНЫЙ КОМБИНАТ': 'АО',\n",
    "        'ОК РУСАЛ ТД': 'АО'\n",
    "    }\n",
    "    org_id_counter = 0\n",
    "    for org_full_name, org_type in map_orgs.items():\n",
    "        ontology_item_ref = OrganizationReferent()\n",
    "        names_to_add_to_ontology = set()\n",
    "        org_full_name_upper = org_full_name.upper()\n",
    "        words = org_full_name_upper.split()\n",
    "        names_to_add_to_ontology.add(org_full_name_upper)\n",
    "        if len(words) > 1 and words[0] == \"РУСАЛ\":\n",
    "            names_to_add_to_ontology.add(\" \".join(words[1:]))\n",
    "        if len(words) > 1:\n",
    "            current_prefix_parts = []\n",
    "            for i in range(len(words) - 1):\n",
    "                current_prefix_parts.append(words[i])\n",
    "                prefix_variant = \" \".join(current_prefix_parts)\n",
    "                if not (prefix_variant == \"РУСАЛ\" and words[0] == \"РУСАЛ\" and len(words) > 1):\n",
    "                    names_to_add_to_ontology.add(prefix_variant)\n",
    "        if org_full_name_upper == \"РУСАЛ\": \n",
    "            names_to_add_to_ontology.add(\"РУСАЛ\")\n",
    "        \n",
    "        for name_variant in names_to_add_to_ontology:\n",
    "            ontology_item_ref.add_slot(OrganizationReferent.ATTR_NAME, name_variant, False)\n",
    "        ontology_item_ref.add_slot(OrganizationReferent.ATTR_TYPE, org_type.upper(), False)\n",
    "        org_ontos.add_referent(f\"org_{org_id_counter}\", ontology_item_ref)\n",
    "        org_id_counter += 1\n",
    "    return org_ontos\n",
    "\n",
    "conf_ontology = _configure_org_ontology()\n",
    "conf_ontology.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "51213f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Проверочный текст продавец ОАО РЖД покупатель РУСАЛ Ачинский Глиноземный Комбинат, АО\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from typing import Dict, List\n",
    "from pullenti.ner.Slot import Slot\n",
    "from pullenti.ner.org.OrganizationReferent import OrganizationReferent\n",
    "\n",
    "ORG_TYPES = {\n",
    "            'АО': 'акционерное общество', \n",
    "            'ОАО': 'открытое акционерное общество',\n",
    "            'ЗАО': 'закрытое акционерное общество',\n",
    "            'ООО': 'общество с ограниченной ответственностью',\n",
    "            'ИП': 'индивидуальный предприниматель', \n",
    "            'ПАО': 'публичное акционерное общество',\n",
    "            'ОО': 'общественная организация',\n",
    "            'НП': 'некоммерческое партнерство',\n",
    "            'ГУП': 'государственное унитарное предприятие',\n",
    "            'МУП': 'муниципальное унитарное предприятие',\n",
    "            'ФГУП': 'федеральное государственное унитарное предприятие',\n",
    "        }\n",
    "\n",
    "_LAT2CYR = str.maketrans({\n",
    "    'A':'А','B':'В','C':'С','E':'Е','H':'Н','K':'К','M':'М','O':'О','P':'Р','T':'Т','X':'Х','Y':'У',\n",
    "    'a':'а','c':'с','e':'е','o':'о','p':'р','x':'х','y':'у','k':'к','m':'м','h':'н','b':'в','t':'т',\n",
    "    'R':'Р','r':'р','V':'В','v':'в'\n",
    "})\n",
    "\n",
    "def _normalize_text(text: str) -> str:\n",
    "    \"\"\"Нормализует текст: лат->кир гомоглифы, нижний регистр, 'ё'->'е', убирает пунктуацию, схлопывает пробелы.\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    # Сначала переводим вероятные латинские буквы в кириллицу (важно до нижнего регистра)\n",
    "    text = text.translate(_LAT2CYR)\n",
    "    text = text.lower().replace('ё', 'е')\n",
    "    # Удаляем пунктуацию (оставляем буквы/цифры/пробелы)\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text, flags=re.UNICODE)\n",
    "    # Схлопываем все виды пробельных в 1 пробел\n",
    "    text = re.sub(r'\\s+', ' ', text, flags=re.UNICODE)\n",
    "    return text.strip()\n",
    "\n",
    "def _normalize_for_match(text: str) -> str:\n",
    "    \"\"\"Нормализация для поиска: только лат->кир (без лоуеркейса/удаления пунктуации).\"\"\"\n",
    "    return text.translate(_LAT2CYR)\n",
    "\n",
    "def _charflex(token: str) -> str:\n",
    "    \"\"\"\n",
    "    Разрешить разрывы внутри слова: 'КОМБИНАТ' -> 'к\\\\s*о\\\\s*м\\\\s*б\\\\s*и\\\\s*н\\\\s*а\\\\s*т'\n",
    "    Работает поверх _normalize_text(token).\n",
    "    \"\"\"\n",
    "    t = _normalize_text(token)\n",
    "    parts = [re.escape(ch) + r'\\s*' for ch in t]\n",
    "    return ''.join(parts).rstrip(r'\\s*')\n",
    "\n",
    "def _expand_to_nearest_quotes(full_text: str, start: int, end: int) -> tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Если match попал внутрь кавычек — расширяем до ближайшей парной кавычки.\n",
    "    Возвращаем (s, e) в координатах исходного текста. Если кавычек нет — исходный span.\n",
    "    \"\"\"\n",
    "    left = max(full_text.rfind('«', 0, start), full_text.rfind('\"', 0, start))\n",
    "    right_1 = full_text.find('»', end)\n",
    "    right_2 = full_text.find('\"', end)\n",
    "    rights = [i for i in (right_1, right_2) if i != -1]\n",
    "    right = min(rights) if rights else -1\n",
    "\n",
    "    if left != -1 and right != -1 and left < start < end < right + 1:\n",
    "        # включим сами кавычки\n",
    "        return left, right + 1\n",
    "    return start, end\n",
    "\n",
    "def _strip_legal_form_edges(text: str) -> str:\n",
    "    \"\"\"Удаляет юр-формы (из ORG_TYPES: и сокращения, и полные фразы) только с краёв строки.\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    # Альтернативы из ORG_TYPES: ключи (АО, ООО, ...) + значения (полные фразы)\n",
    "    alts = [re.escape(k) for k in ORG_TYPES.keys()]\n",
    "    alts += [re.escape(v) for v in ORG_TYPES.values() if v]\n",
    "    if not alts:\n",
    "        return text.strip()\n",
    "\n",
    "    ALT = r'(?:' + '|'.join(sorted(alts, key=len, reverse=True)) + r')'\n",
    "    s = text.strip()\n",
    "\n",
    "    # Нормализуем латиницу в кириллицу (индексы не меняются — посимвольная замена)\n",
    "    s = s.translate(_LAT2CYR)\n",
    "\n",
    "    # Срез слева (итеративно)\n",
    "    while True:\n",
    "        m = re.match(r'^(?:[«\"\\(\\']*\\s*)' + ALT + r'(?:\\s+|\\s*[-–—]\\s*)', s, flags=re.IGNORECASE)\n",
    "        if not m:\n",
    "            break\n",
    "        s = s[m.end():].lstrip()\n",
    "\n",
    "    # Срез справа (итеративно)\n",
    "    while True:\n",
    "        m = re.search(r'(?:\\s+|\\s*[-–—]\\s*)' + ALT + r'(?:[»\"\\)\\']*\\s*)$', s, flags=re.IGNORECASE)\n",
    "        if not m:\n",
    "            break\n",
    "        s = s[:m.start()].rstrip()\n",
    "\n",
    "    return s.strip()\n",
    "\n",
    "_MISC_TAIL_RE = re.compile(\n",
    "    r'[\\s,]*\\(?\\s*(?:ИНН|КПП|ОГРН|ОКПО)\\s*[:№]?\\s*[\\d/\\-\\s]+\\)?\\s*$',\n",
    "    flags=re.IGNORECASE | re.UNICODE\n",
    ")\n",
    "\n",
    "def _strip_trailing_props(s: str) -> str:\n",
    "    \"\"\"Убирает с конца '(... ИНН ...)', ', ИНН:...', 'КПП ...' и т.п. Повторяет до полного удаления.\"\"\"\n",
    "    if not s:\n",
    "        return s\n",
    "    out = s\n",
    "    while True:\n",
    "        new = _MISC_TAIL_RE.sub('', out)\n",
    "        if new == out:\n",
    "            break\n",
    "        out = new\n",
    "    # подчистим висящие кавычки/знаки после удаления хвоста\n",
    "    out = out.strip().strip('«»\"“”„\\' ,;:')\n",
    "    return out\n",
    "\n",
    "def _best_surface_from_occurrence(org: OrganizationReferent, full_text: str) -> str:\n",
    "    \"\"\"Возвращает самую длинную форму из фактических вхождений в тексте (сохраняем дефисы), без юр-форм по краям.\"\"\"\n",
    "    occs = getattr(org, 'occurrence', None)\n",
    "    if not occs:\n",
    "        return \"\"\n",
    "    spans = []\n",
    "    for occ in occs:\n",
    "        b = getattr(occ, 'begin_char', None)\n",
    "        e = getattr(occ, 'end_char', None)\n",
    "        if isinstance(b, int) and isinstance(e, int) and 0 <= b <= e < len(full_text):\n",
    "            spans.append(full_text[b:e+1])\n",
    "\n",
    "    if not spans:\n",
    "        return \"\"\n",
    "\n",
    "    # Берём самую «содержательную» (по длине после нормализации)\n",
    "    surface = max(spans, key=lambda s: len(_normalize_text(s)))\n",
    "    surface = _strip_legal_form_edges(surface)\n",
    "    surface = _strip_trailing_props(surface) \n",
    "    surface = re.sub(r'\\s+', ' ', surface).strip()\n",
    "    return surface.upper()\n",
    "\n",
    "def _match_surface_in_text_preserve_punct(name: str, full_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Ищем NAME в тексте, разрешая:\n",
    "      - пробелы/дефисы между токенами,\n",
    "      - пробелы ВНУТРИ слов (OCR-разрывы),\n",
    "      - лат<->кир гомоглифы.\n",
    "    Возвращаем форму из исходного текста (с дефисами/кавычками), срезав юр-формы и хвосты реквизитов.\n",
    "    \"\"\"\n",
    "    if not name:\n",
    "        return \"\"\n",
    "    tokens = _normalize_text(name).split()\n",
    "    if not tokens:\n",
    "        return \"\"\n",
    "\n",
    "    # Готовим шаблон: внутри слова — \\s*, между словами — (пробел|дефис)+\n",
    "    token_patterns = [_charflex(tok) for tok in tokens]\n",
    "    sep = r'(?:\\s|[-–—])+'\n",
    "    pattern = r'\\b' + sep.join(token_patterns) + r'\\b'\n",
    "\n",
    "    text4match = _normalize_for_match(full_text)\n",
    "    m = re.search(pattern, text4match, flags=re.IGNORECASE | re.UNICODE)\n",
    "    if not m:\n",
    "        return \"\"\n",
    "\n",
    "    # Базовый span по нормализованному тексту → те же индексы в исходном тексте\n",
    "    s, e = m.start(), m.end()\n",
    "\n",
    "    # Если внутри кавычек — аккуратно расширим до границ кавычек\n",
    "    s, e = _expand_to_nearest_quotes(full_text, s, e)\n",
    "\n",
    "    surface = full_text[s:e]\n",
    "    surface = _strip_legal_form_edges(surface)\n",
    "    surface = _strip_trailing_props(surface)\n",
    "    surface = re.sub(r'\\s+', ' ', surface).strip()\n",
    "    return surface.upper()\n",
    "\n",
    "_CYRILLIC_RE = re.compile(r'[а-яё]', flags=re.IGNORECASE)\n",
    "\n",
    "def _looks_valid_org_name(name: str) -> bool:\n",
    "    n = (name or \"\").strip()\n",
    "    if len(n) < 3:\n",
    "        return False\n",
    "    # Наличие кириллицы (после твоей лат->кир нормализации это надёжный фильтр от AO/OT и пр.)\n",
    "    if not _CYRILLIC_RE.search(n):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def select_best_org_name_and_type(org: OrganizationReferent, full_text: str):\n",
    "    \"\"\"\n",
    "    Приоритет:\n",
    "      1) Самая длинная «поверхность» по NAME-слотам, найденная в тексте (с дефисами/тире).\n",
    "      2) Лучшая «поверхность» из occurrence (если 1) не сработал).\n",
    "      3) Фоллбэк: самая длинная валидная NAME-строка из слотов.\n",
    "    Везде обрезаем юр-формы по краям и хвосты реквизитов.\n",
    "    \"\"\"\n",
    "    # 1) кандидаты из NAME, реально найденные в тексте\n",
    "    name_surfaces: list[str] = []\n",
    "    for s in org.slots:\n",
    "        if s.type_name != OrganizationReferent.ATTR_NAME:\n",
    "            continue\n",
    "        raw = str(s.value).strip()\n",
    "        if not _looks_valid_org_name(raw):\n",
    "            continue\n",
    "        core = _strip_legal_form_edges(raw)\n",
    "        if not core:\n",
    "            continue\n",
    "        surface = _match_surface_in_text_preserve_punct(core, full_text)\n",
    "        if surface and _looks_valid_org_name(surface):\n",
    "            name_surfaces.append(surface)\n",
    "\n",
    "    best_name = \"\"\n",
    "    if name_surfaces:\n",
    "        # Берём самую «содержательную» по длине нормализованного текста\n",
    "        best_name = max(name_surfaces, key=lambda s: len(_normalize_text(s)))\n",
    "\n",
    "    # 2) occurrence как резерв\n",
    "    if not best_name:\n",
    "        occ = _best_surface_from_occurrence(org, full_text)\n",
    "        if _looks_valid_org_name(occ):\n",
    "            best_name = occ\n",
    "\n",
    "    # 3) фоллбэк — самая длинная валидная NAME-строка\n",
    "    if not best_name:\n",
    "        max_len = 0\n",
    "        for s in org.slots:\n",
    "            if s.type_name == OrganizationReferent.ATTR_NAME:\n",
    "                raw = _strip_trailing_props(_strip_legal_form_edges(str(s.value).strip().upper()))\n",
    "                if _looks_valid_org_name(raw) and len(raw) > max_len:\n",
    "                    best_name, max_len = raw, len(raw)\n",
    "\n",
    "    # Тип по словарю\n",
    "    best_type = \"\"\n",
    "    for s in org.slots:\n",
    "        if s.type_name == OrganizationReferent.ATTR_TYPE:\n",
    "            type_value = str(s.value).upper()\n",
    "            if type_value in ORG_TYPES:\n",
    "                best_type = type_value\n",
    "                break\n",
    "\n",
    "    # финальная подчистка кавычек/пробелов\n",
    "    best_name = best_name.strip().strip('«»\"“”„\\'')\n",
    "    return {\"name\": best_name, \"type\": best_type}\n",
    "\n",
    "def candidates_org(text: str) -> List[Dict]:\n",
    "    with ProcessorService.create_specific_processor(OrganizationAnalyzer.ANALYZER_NAME) as proc:\n",
    "        ar: AnalysisResult = proc.process(SourceOfAnalysis(text), conf_ontology)\n",
    "    candidates = []\n",
    "    for e0_ in ar.entities:\n",
    "\n",
    "        if not isinstance(e0_, OrganizationReferent):\n",
    "            continue\n",
    "        best_name = select_best_org_name_and_type(e0_, text)\n",
    "        candidates.append(best_name)\n",
    "    if len(candidates) > 1: \n",
    "        candidates = candidates[:2]\n",
    "    else:\n",
    "        raise ValueError(\"Не удалось найти подходящие организации как миниму должно быть 2 организации\")\n",
    "    return candidates\n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "86a29adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Проверочный текст продавец ФРЕЙТ ЛИНК, АО покупатель ОК РУСАЛ ТД, АО\n",
      "Кандидаты: [{'name': 'ФРЕЙТ ЛИНК', 'type': 'АО'}, {'name': 'ОК РУСАЛ ТД', 'type': 'АО'}]\n",
      "Продавец: ФРЕЙТ ЛИНК\n",
      "Покупатель: ОК РУСАЛ ТД\n"
     ]
    }
   ],
   "source": [
    "def _contains_rusal(name: str) -> bool:\n",
    "    \"\"\"Проверка 'РУСАЛ' в названии с нормализацией.\"\"\"\n",
    "    return 'русал' in _normalize_text(name)\n",
    "\n",
    "def _norm_index_of_phrase(phrase: str, norm_text: str) -> int:\n",
    "    \"\"\"Ищем индекс фразы в НОРМАЛИЗОВАННОМ тексте.\"\"\"\n",
    "    if not phrase:\n",
    "        return -1\n",
    "    norm_phrase = _normalize_text(phrase)\n",
    "    return norm_text.find(norm_phrase)\n",
    "\n",
    "def _keyword_positions(norm_text: str, keywords: list[str]) -> list[int]:\n",
    "    \"\"\"Собираем позиции ключевых слов в НОРМАЛИЗОВАННОМ тексте (с поддержкой окончаний).\"\"\"\n",
    "    positions = []\n",
    "    for kw in keywords:\n",
    "        kw_norm = _normalize_text(kw)\n",
    "        # Лёгкая лемматизация по корню для покупатель/продавец\n",
    "        if kw_norm == 'покупатель':\n",
    "            pattern = r'\\bпокупател\\w*\\b'\n",
    "        elif kw_norm == 'продавец':\n",
    "            pattern = r'\\bпродавц\\w*\\b'\n",
    "        else:\n",
    "            pattern = r'\\b' + re.escape(kw_norm) + r'\\b'\n",
    "        positions.extend(m.start() for m in re.finditer(pattern, norm_text, flags=re.IGNORECASE))\n",
    "    return positions\n",
    "\n",
    "def _nearest_distance(pos: int, positions: list[int]) -> float:\n",
    "    if pos < 0 or not positions:\n",
    "        return float('inf')\n",
    "    return min(abs(pos - p) for p in positions)\n",
    "\n",
    "def assign_roles_from_candidates(\n",
    "    text: str,\n",
    "    orgs: list[dict],  # [{'name': str, 'type': str}, ...] — возьмём только первые 2\n",
    "    seller_key_words: list[str],\n",
    "    buyer_key_words: list[str],\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Возвращает {'seller': <name>, 'buyer': <name>} по правилам:\n",
    "      1) 'РУСАЛ' -> покупатель\n",
    "      2) иначе — по близости ключевых слов\n",
    "      3) иначе — фолбэк: [0]=seller, [1]=buyer\n",
    "    \"\"\"\n",
    "    result = {'seller': '', 'buyer': ''}\n",
    "\n",
    "    # Берём только первых двух кандидатов\n",
    "    cand = [o for o in orgs if o.get('name')][:2]\n",
    "    if not cand:\n",
    "        return result\n",
    "    if len(cand) == 1:\n",
    "        # Если только одна компания, без двусмысленности ставим её как продавца (можно поменять под ваш кейс)\n",
    "        result['seller'] = cand[0]['name']\n",
    "        return result\n",
    "\n",
    "    A, B = cand[0]['name'], cand[1]['name']\n",
    "\n",
    "    # Правило 1: \"РУСАЛ\" — покупатель\n",
    "    a_is_rusal = _contains_rusal(A)\n",
    "    b_is_rusal = _contains_rusal(B)\n",
    "    if a_is_rusal ^ b_is_rusal:\n",
    "        result['buyer']  = A if a_is_rusal else B\n",
    "        result['seller'] = B if a_is_rusal else A\n",
    "        return result\n",
    "    elif a_is_rusal and b_is_rusal:\n",
    "        # Оба содержат 'РУСАЛ': покупателем делаем более длинное имя (менее амбигуно)\n",
    "        if len(_normalize_text(A)) >= len(_normalize_text(B)):\n",
    "            result['buyer'], result['seller'] = A, B\n",
    "        else:\n",
    "            result['buyer'], result['seller'] = B, A\n",
    "        return result\n",
    "\n",
    "    # Правило 2: по ключевым словам (ищем ближайшие)\n",
    "    norm_text = _normalize_text(text)\n",
    "\n",
    "    # Пытаемся взять \"поверхность\" имен из текста, чтобы точнее найти позицию\n",
    "    A_surface = _match_surface_in_text_preserve_punct(A, text) or A\n",
    "    B_surface = _match_surface_in_text_preserve_punct(B, text) or B\n",
    "\n",
    "    posA = _norm_index_of_phrase(A_surface, norm_text)\n",
    "    posB = _norm_index_of_phrase(B_surface, norm_text)\n",
    "\n",
    "    buyer_pos = _keyword_positions(norm_text, buyer_key_words)\n",
    "    seller_pos = _keyword_positions(norm_text, seller_key_words)\n",
    "\n",
    "    A_b = _nearest_distance(posA, buyer_pos)\n",
    "    B_b = _nearest_distance(posB, buyer_pos)\n",
    "    A_s = _nearest_distance(posA, seller_pos)\n",
    "    B_s = _nearest_distance(posB, seller_pos)\n",
    "\n",
    "    # Если вообще нет ключевых слов — фолбэк\n",
    "    if all(d == float('inf') for d in (A_b, B_b, A_s, B_s)):\n",
    "        result['seller'], result['buyer'] = A, B\n",
    "        return result\n",
    "\n",
    "    # Покупателем считаем того, кто ближе к \"buyer\"-ключам (при равенстве — тот, кто ДАЛЬШЕ от \"seller\"-ключей)\n",
    "    if (A_b < B_b) or (A_b == B_b and A_s > B_s):\n",
    "        result['buyer'], result['seller'] = A, B\n",
    "    elif (B_b < A_b) or (A_b == B_b and B_s > A_s):\n",
    "        result['buyer'], result['seller'] = B, A\n",
    "    else:\n",
    "        # Если всё равно неоднозначно — смотрим \"seller\"-близость\n",
    "        if A_s < B_s:\n",
    "            result['seller'], result['buyer'] = A, B\n",
    "        elif B_s < A_s:\n",
    "            result['seller'], result['buyer'] = B, A\n",
    "        else:\n",
    "            # финальный фолбэк\n",
    "            result['seller'], result['buyer'] = A, B\n",
    "\n",
    "    return result\n",
    "\n",
    "seller_key_words = ['продавец', 'с одной стороны', 'между', 'от продавца']\n",
    "buyer_key_words  = ['покупатель', 'с другой стороны', 'от покупателя']\n",
    "\n",
    "sample = TEST_TEXT_LIST[2]\n",
    "text =  sample.text_raw\n",
    "seller = sample.seller\n",
    "buyer = sample.buyer\n",
    "print(f'Проверочный текст продавец {seller} покупатель {buyer}')\n",
    "candidates = candidates_org(text)\n",
    "print(f'Кандидаты: {candidates}')\n",
    "\n",
    "roles = assign_roles_from_candidates(text, candidates, seller_key_words, buyer_key_words)\n",
    "print('Продавец:', roles['seller'])\n",
    "print('Покупатель:', roles['buyer'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.9.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
